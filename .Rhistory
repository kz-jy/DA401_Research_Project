results <- matrix(0, nrow = length(labels), ncol = dimension)
for (i in 1:length(labels))
results[i, labels[[i]]] <- 1
results
}
one_hot_raceTrain <- to_one_hot(train_perf$race..ethnicity,5)
one_hot_pleTrain <- to_one_hot(train_perf$parental.level.of.education,6)
val_indices <- 1:140
val_race <- one_hot_raceTrain[val_indices,]
partial_x_train <- one_hot_raceTrain[-val_indices,]
val_ple<- one_hot_pleTrain[val_indices,]
partial_y_train = one_hot_pleTrain[-val_indices,]
KSmodel_perf <- keras_model_sequential() %>%
layer_dense(units = 8, activation = "relu", input_shape = 5) %>%
layer_dense(units = 8, activation = "relu") %>%
layer_dense(units = 6, activation = "softmax")
KSmodel_perf %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy"))
KSmodel_perf %>%
fit(x = partial_x_train,
y = partial_y_train, epochs= 5,
validation_data = list(val_race, val_ple))
one_hot_raceTest <- to_one_hot(test_perf$race..ethnicity,5)
one_hot_pleTrain <- to_one_hot(test_perf$parental.level.of.education,6)
PLE_ksmodel = KSmodel_perf %>%
predict(one_hot_raceTest)
PLE_ksmodel<-data.frame(PLE_ksmodel)
PLE_ks<-colnames(PLE_ksmodel)[apply(PLE_ksmodel,1,which.max)]
table(PLE_ks)
KSmodel_perf <- keras_model_sequential() %>%
layer_dense(units = 8, activation = "relu", input_shape = 5) %>%
layer_dense(units = 8, activation = "relu") %>%
layer_dense(units = 6, activation = "softmax")
KSmodel_perf %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy"))
KSmodel_perf %>%
fit(x = partial_x_train,
y = partial_y_train, epochs= 5,
validation_data = list(val_race, val_ple))
one_hot_raceTest <- to_one_hot(test_perf$race..ethnicity,5)
one_hot_pleTrain <- to_one_hot(test_perf$parental.level.of.education,6)
PLE_ksmodel = KSmodel_perf %>%
predict(one_hot_raceTest)
PLE_ksmodel<-data.frame(PLE_ksmodel)
PLE_ks<-colnames(PLE_ksmodel)[apply(PLE_ksmodel,1,which.max)]
table(PLE_ks)
KSmodel_perf <- keras_model_sequential() %>%
layer_dense(units = 8, activation = "relu", input_shape = 5) %>%
layer_dense(units = 8, activation = "relu") %>%
layer_dense(units = 6, activation = "softmax")
KSmodel_perf %>%
compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy"))
KSmodel_perf %>%
fit(x = partial_x_train,
y = partial_y_train, epochs= 5,
validation_data = list(val_race, val_ple))
one_hot_raceTest <- to_one_hot(test_perf$race..ethnicity,5)
one_hot_pleTrain <- to_one_hot(test_perf$parental.level.of.education,6)
PLE_ksmodel = KSmodel_perf %>%
predict(one_hot_raceTest)
PLE_ksmodel<-data.frame(PLE_ksmodel)
PLE_ks<-colnames(PLE_ksmodel)[apply(PLE_ksmodel,1,which.max)]
table(PLE_ks)
PLE_ksmodel
TPC
KS_TPC <- keras_model_sequential() %>%
layer_flatten(input_shape = c(1)) %>%
layer_dense(units = 3, activation = "linear") %>%
layer_dense(units = 1, activation = "sigmoid")
KS_TPC %>%
compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy")
KS_TPC %>%
fit(x = as.matrix(as.integer(train_perf[,"race..ethnicity"])),
y = train_perf[,"test.preparation.course"],epochs = 5,validation_split = 0.3,verbose = 2)
TPC_ksmodel = KS_TPC %>%
predict(as.matrix(as.integer(test_perf[,"race..ethnicity"])))
TPC = ifelse(TPC_ksmodel >= mean(train_perf$test.preparation.course),1,0)
TPC
table(TPC)
table(test_perf$test.preparation.course)
glm(test.preparation.course~race..ethnicity,data=train_perf,family = "binomial")
predict(a,newdata=test_perf)
a<-glm(test.preparation.course~race..ethnicity,data=train_perf,family = "binomial")
predict(a,newdata=test_perf)
table(predict(a,newdata=test_perf))
predict(a,newdata=test_perf)
table(train_perf$test.preparation.course)
table(Perf$test.preparation.course)
TPC_log
TPC_log<-glm(test.preparation.course~race..ethnicity,data=train_perf,family = "binomial")
table(predict(TPC_log,newdata=test_perf))
ifelse(predict(TPC_log,newdata=test_perf)>-0.532,1,0)
TPC = ifelse(predict(TPC_log,newdata=test_perf)>-0.532,1,0)
table(TPC)
lunch_log<-glm(lunch~race..ethnicity,data=train_perf,family = "binomial")
predict(lunch_log,newdata=test_perf)
table(predict(lunch_log,newdata=test_perf))
table(Perf$lunch)
lunch = ifelse(predict(lunch_log,newdata=test_perf)>-0.52,1,0)
table(lunch)
table(PLE_ks)
predictions<-data.frame(PLE_ks,TPC,lunch)
View(predictions)
PLE_ksmodel = KSmodel_perf %>%
predict(one_hot_raceTest)
PLE_ksmodel<-data.frame(PLE_ksmodel)
colnames(PLE_ksmodel)<-c(1,2,3,4,5,6)
PLE_ks<-colnames(PLE_ksmodel)[apply(PLE_ksmodel,1,which.max)]
PLE_ks
predictions<-data.frame(PLE_ks,TPC,lunch)
View(predictions)
predictions<-data.frame(PLE_ks,TPC,lunch,test_perf$race..ethnicity)
View(predictions)
predictions$race<-predictions$test_perf.race..ethnicity
View(predictions)
predictions<-data.frame(PLE_ks,TPC,lunch,race=test_perf$race..ethnicity)
View(predictions)
ctrl_rf = trainControl(method = "oob")
rf_perf= train(math.score ~., data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
library(caret)
rf_perf= train(math.score ~., data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
rf_perf= train(math.score ~ ., data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
rf_perf= train(math.score ~ PLE_ks+TPC+lunch+race, data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
rf_perf= train(math.score ~ PLE_ks+TPC+lunch+race, data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
library(caret)
rf_perf= train(math.score ~ PLE_ks+TPC+lunch+race, data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
rf_perf= train(math.score ~ PLE_ks+TPC+lunch+race, data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
rf_perf= train(math.score ~ PLE_ks+TPC+lunch+race, data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
require(caret)
rf_perf= train(math.score ~ PLE_ks+TPC+lunch+race, data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
install.packages('e1071')
rf_perf= train(math.score ~ PLE_ks+TPC+lunch+race, data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
rf_perf= train(math.score ~ PLE_ks+TPC+lunch+race, data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
ctrl_rf = trainControl(method = "oob")
rf_perf= train(math.score ~ PLE_ks+TPC+lunch+race, data = predictions, method = "rf", trControl = ctrl_rf,
tuneLength =5, ntree = 10, importance = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(dplyr)
library(dplyr)
library(ggplot2)
library(rpart)
library(Rcpp)
install.packages(c("caret", "dplyr", "ggplot2", "ggpubr", "interactions", "jtools", "keras", "rstatix", "tensorflow", "tidyverse"))
library(dplyr)
setwd("~/Documents/GitHub/DA401_SuicideRisk_research")
load("NSDUH_2020.RData")
load("NSDUH_2019.RData")
load("NSDUH_2018.RData")
#Data cleaning
true20<- NSDUH_2020%>%
filter(auinpyr == 1|auoptyr == 1)
true19<-PUF2019_100920 %>%
filter(auinpyr == 1|auoptyr == 1)
true18<-PUF2018_100819 %>%
filter(auinpyr == 1|auoptyr == 1)
true18$QUESTID2<-as.integer(true18$QUESTID2)
true19$QUESTID2<-as.integer(true19$QUESTID2)
SuicideRisk<-bind_rows(true20, true19, true18)
SR<-SuicideRisk %>%
dplyr::select(DSTHOP30, mhsuithk, mhsuipln, mhsuitry, K6SCMON, SMIPP_U, AUNMPSY2, AUNMPGE2, AUNMMED2, AUNMAHS2, AUNMRES2,
AUNMSFA2, AUNMMEN2, AUNMTHE2, AUNMDOC2, AUNMCLN2, AUNMDTM2, AUNMOTO2, amdelt, ASDSOVL2, CATAG6, irmarit,
irsex, IREDUHIGHST2, NEWRACE2, wrkdpstyr, income, ALCWD2SX, alcndmor)
val1 = c(83,91,93,94,97,98,99) #value for the invalid data (unknow, no repsonse...)
SR <- as.data.frame(sapply(SR, function(x) replace(x, x %in% val1, 0)))
#Assign risk level
SR<-SR %>% mutate(risk_level = case_when(as.numeric(DSTHOP30 != 98 & DSTHOP30<3) + as.numeric(mhsuithk==1)+
as.numeric(mhsuipln == 1) + as.numeric(mhsuitry == 1) +
as.numeric(K6SCMON > 16) + as.numeric(SMIPP_U > 0.65) >= 3 ~"high",
as.numeric(DSTHOP30 != 98 & DSTHOP30 == 3) + as.numeric(mhsuithk == 1) +
as.numeric(mhsuipln == 1 | mhsuitry == 1) +
as.numeric(K6SCMON <= 16 & K6SCMON >= 8) +
as.numeric(SMIPP_U <= 0.65 & SMIPP_U > 0.3) >= 3 ~"moderate",
as.numeric(DSTHOP30 != 98 & DSTHOP30>3) + as.numeric(K6SCMON < 8) +
as.numeric(mhsuithk== 1 | mhsuipln == 1 | mhsuitry == 1) +
as.numeric(SMIPP_U <= 0.3) >= 1 ~ "low"))
SR$risk_level[is.na(SR$risk_level)] <- "no"
load("NSDUH_2018.RData")
#Data cleaning
true20<- NSDUH_2020%>%
filter(auinpyr == 1|auoptyr == 1)
View(SR)
install.packages("ElemStatLearn")
library(dplyr)
library(ggplot2)
library(caret)
library(rpart)
#### Training Testing set
SR<-SR[c(-7:-18)]
SR<-SuicideRisk %>%
dplyr::select(DSTHOP30, mhsuithk, mhsuipln, mhsuitry, K6SCMON, SMIPP_U, AUNMPSY2, AUNMPGE2, AUNMMED2, AUNMAHS2, AUNMRES2,
AUNMSFA2, AUNMMEN2, AUNMTHE2, AUNMDOC2, AUNMCLN2, AUNMDTM2, AUNMOTO2, amdelt, ASDSOVL2, CATAG6, irmarit,
irsex, IREDUHIGHST2, NEWRACE2, wrkdpstyr, income, ALCWD2SX, alcndmor)
val1 = c(83,91,93,94,97,98,99) #value for the invalid data (unknow, no repsonse...)
SR <- as.data.frame(sapply(SR, function(x) replace(x, x %in% val1, 0)))
#Assign risk level
SR<-SR %>% mutate(risk_level = case_when(as.numeric(DSTHOP30 != 98 & DSTHOP30<3) + as.numeric(mhsuithk==1)+
as.numeric(mhsuipln == 1) + as.numeric(mhsuitry == 1) +
as.numeric(K6SCMON > 16) + as.numeric(SMIPP_U > 0.65) >= 3 ~"high",
as.numeric(DSTHOP30 != 98 & DSTHOP30 == 3) + as.numeric(mhsuithk == 1) +
as.numeric(mhsuipln == 1 | mhsuitry == 1) +
as.numeric(K6SCMON <= 16 & K6SCMON >= 8) +
as.numeric(SMIPP_U <= 0.65 & SMIPP_U > 0.3) >= 3 ~"moderate",
as.numeric(DSTHOP30 != 98 & DSTHOP30>3) + as.numeric(K6SCMON < 8) +
as.numeric(mhsuithk== 1 | mhsuipln == 1 | mhsuitry == 1) +
as.numeric(SMIPP_U <= 0.3) >= 1 ~ "low"))
SR$risk_level[is.na(SR$risk_level)] <- "no"
#Combine treatment from multiple facilities into a single var
val2 = c(985,994,997,998,999)
SR[7:18] <- sapply(SR[7:18], function(x) replace(x, x %in% val2, 0))
SR$inpatient<-rowSums(SR[7:12])
SR$outpatient <- rowSums(SR[13:18])
#### Training Testing set
SR<-SR[c(-7:-18)]
SR<-SR[c(1:17, 19, 20, 18)]
SR<-na.omit(SR[c(1:7,9:20)]) #omit the variable with too much NA and all rows w/ NA (drop ASDSOVL2)
SR$risk_level<-as.factor(SR$risk_level)
set.seed(1)
index = createDataPartition(y=SR$risk_level, p=0.7, list=FALSE)
sr_train =SR[index,]
sr_test = SR[-index,]
#Decision Tress
sr_train<-sr_train[c(7:19)]
ctrl_dt = trainControl(method="repeatedcv",number=10, repeats=5)
dtree = train(risk_level~ ., data = sr_train, method = "rpart", trControl = ctrl_dt,
tuneLength = 30)
prp(dtree$finalModel)
library(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
prp(dtree$finalModel)
library(keras)
library(tensorflow)
#Neural Network
nn <- keras_model_sequential() %>%
layer_flatten(input_shape = c(12)) %>%
layer_dense(units = 8, activation = "linear")%>%
layer_dense(units = 3, activation = "softmax")
nn%>% compile(
loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy")
nn%>% fit(x = as.matrix(sr_train[,c(1:18)]),
y = st_train[,19],epochs = 5,validation_split = 0.3,verbose = 2)
nn%>% fit(x = as.matrix(sr_train[,c(1:12)]),
y = st_train[,13],epochs = 5,validation_split = 0.3,verbose = 2)
nn%>% fit(x = as.matrix(sr_train[,c(1:12)]),
y = sr_train[,13],epochs = 5,validation_split = 0.3,verbose = 2)
class(sr_train[,13])
nn %>% fit(x = as.matrix(sr_train[,c(1:12)]),
y = sr_train[,"risk_level"], epochs = 5, validation_split = 0.3,verbose = 2)
class(as.matrix(sr_train[,c(1:12)]))
class(sr_train[,"risk_level"])
nn %>% fit(x = as.matrix(sr_train[,c(1:12)]),
y = sr_train$risk_level, epochs = 5, validation_split = 0.3,verbose = 2)
nn %>% fit(x = as.matrix(sr_train[,c(1:12)]),
y = sr_train[,13], epochs = 5, validation_split = 0.3,verbose = 2)
nn %>% fit(x = as.matrix(sr_train[,c(1:12)]),
y = sr_train[,c(13)], epochs = 5, validation_split = 0.3,verbose = 2)
View(sr_train)
#Neural Network
train_nn<-sr_train
train_nn$risk_level<-factor(train_nn$risk_level, levels = c("no","low","moderate","high"),
labels = c(1,2,3,4))
nn %>% fit(x = as.matrix(sr_train[,c(1:12)]),
y = sr_train[,"risk_level"], epochs = 5, validation_split = 0.3,verbose = 2)
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = train_nn[,"risk_level"], epochs = 5, validation_split = 0.3,verbose = 2)
class(train_nn$risk_level)
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = as.matrix(train_nn[,"risk_level"]), epochs = 5, validation_split = 0.3,verbose = 2)
View(sr_train)
nn <- keras_model_sequential() %>%
layer_flatten(input_shape = c(12)) %>%
layer_dense(units = 8, activation = "linear")%>%
layer_dense(units = 3, activation = "softmax")
nn%>% compile(
loss = "categorical_crossentropy",
optimizer = 'adam',
metrics = 'accuracy')
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = as.matrix(train_nn[,"risk_level"]), epochs = 5, validation_split = 0.3,verbose = 2)
nn <- keras_model_sequential() %>%
layer_flatten(input_shape = c(12)) %>%
layer_dense(units = 8, activation = "linear")%>%
layer_dense(units = 3, activation = "Softmax")
nn%>% compile(
loss = "categorical_crossentropy",
optimizer = 'adam',
metrics = 'accuracy')
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = as.matrix(train_nn[,"risk_level"]), epochs = 5, validation_split = 0.3,verbose = 2)
nn <- keras_model_sequential() %>%
layer_flatten(input_shape = c(12)) %>%
layer_dense(units = 8, activation = "linear")%>%
layer_dense(units = 4, activation = "softmax")
nn%>% compile(
loss = "categorical_crossentropy",
optimizer = 'adam',
metrics = 'accuracy')
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = as.matrix(train_nn[,"risk_level"]), epochs = 5, validation_split = 0.3,verbose = 2)
nn <- keras_model_sequential() %>%
layer_flatten(input_shape = c(12)) %>%
layer_dense(units = 8, activation = "linear")%>%
layer_dense(units = 1, activation = "softmax")
nn%>% compile(
loss = "categorical_crossentropy",
optimizer = 'adam',
metrics = 'accuracy')
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = as.matrix(train_nn[,"risk_level"]), epochs = 5, validation_split = 0.3,verbose = 2)
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y =train_nn[,"risk_level"], epochs = 5, validation_split = 0.3,verbose = 2)
View(train_nn)
#Neural Network
train_nn<-sr_train
nn <- keras_model_sequential() %>%
layer_flatten(input_shape = c(12)) %>%
layer_dense(units = 8, activation = "linear")%>%
layer_dense(units = 1, activation = "softmax")
nn%>% compile(
loss = "categorical_crossentropy",
optimizer = 'adam',
metrics = 'accuracy')
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = as.matrix(train_nn[,"risk_level"]), epochs = 5, validation_split = 0.3,verbose = 2)
m<-as.numerice(train_nn$risk_level)
m<-as.numeric(train_nn$risk_level)
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = to_categorical(matrix(train_nn[,"risk_level"])),
epochs = 5, validation_split = 0.3,verbose = 2)
m<-to_categorical(matrix(train_nn[,"risk_level"]))
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = to_categorical(matrix(as.numeric(train_nn[,"risk_level"]))),
epochs = 5, validation_split = 0.3,verbose = 2)
m<-to_categorical(matrix(as.numeric(train_nn[,"risk_level"])))
m<-to_categorical(matrix(as.numeric(train_nn[,"risk_level"])))
m<-to_categorical(matrix(as.numeric(train_nn[,"risk_level"]-1)))
m<-to_categorical(matrix(as.numeric(train_nn[,"risk_level"])-1))
View(m)
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = to_categorical(matrix(as.numeric(train_nn[,"risk_level"])-1)),
epochs = 5, validation_split = 0.3,verbose = 2)
nn <- keras_model_sequential() %>%
layer_flatten(input_shape = c(12)) %>%
layer_dense(units = 8, activation = "linear")%>%
layer_dense(units = 4, activation = "softmax")
nn%>% compile(
loss = "categorical_crossentropy",
optimizer = 'adam',
metrics = 'accuracy')
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = to_categorical(matrix(as.numeric(train_nn[,"risk_level"])-1)),
epochs = 5, validation_split = 0.3,verbose = 2)
nn <- keras_model_sequential() %>%
layer_flatten(input_shape = c(12)) %>%
layer_dense(units = 8, activation = "linear")%>%
layer_dense(units = 4, activation = "softmax")
nn%>% compile(
loss = "categorical_crossentropy",
optimizer = 'adam',
metrics = 'accuracy')
nn %>% fit(x = as.matrix(train_nn[,c(1:12)]),
y = to_categorical(matrix(as.numeric(train_nn[,"risk_level"])-1)),
epochs = 8, validation_split = 0.3,verbose = 2)
#Naive Bayes
ctrl = trainControl(method="repeatedcv", number = 10, repeats = 5)
nb = train(sr_train[c(1:12)], sr_train$risk_level, method = 'nb', trControl=ctrl)
library(caret)
nb = train(sr_train[c(1:12),], sr_train$risk_level, method = 'nb', trControl=ctrl)
nb = train(sr_train[1:12], sr_train$risk_level, method = 'nb', trControl=ctrl)
#Attempts on different model
train_nb<-train_nn
#Attempts on different model
train_nb<-sr_train
#Attempts on different model
train_nb<-sr_train
train_nb$risk_level<-factor(train_nb$risk_level, levels = c("no","low","moderate","high"),
labels = c(1,2,3,4))
nb = train(sr_train[1:12], sr_train$risk_level, method = 'nb', trControl=ctrl)
dtree
#Random Forest
ctrl_rf = trainControl(method = "oob")
rf = train(risk_level ~., data = sr_train, method = "rf", trControl = ctrl_rf,
tuneLength = 8, ntree = 150, importance = TRUE)
library(caret)
#Random Forest
ctrl_rf = trainControl(method = "oob")
rf = train(risk_level ~., data = sr_train, method = "rf", trControl = ctrl_rf,
tuneLength = 8, ntree = 150, importance = TRUE)
rf = caret::train(risk_level ~., data = sr_train, method = "rf", trControl = ctrl_rf,
tuneLength = 8, ntree = 150, importance = TRUE)
prp(rf$finalModel)
nn
View(sr_train)
SR<-SuicideRisk %>%
dplyr::select(DSTHOP30, mhsuithk, mhsuipln, mhsuitry, K6SCMON, SMIPP_U, AUNMPSY2, AUNMPGE2, AUNMMED2, AUNMAHS2, AUNMRES2,
AUNMSFA2, AUNMMEN2, AUNMTHE2, AUNMDOC2, AUNMCLN2, AUNMDTM2, AUNMOTO2, amdelt, ASDSOVL2, CATAG6, irmarit,
irsex, IREDUHIGHST2, NEWRACE2, wrkdpstyr, income, ALCWD2SX, alcndmor)
val1 = c(83,91,93,94,97,98,99) #value for the invalid data (unknow, no repsonse...)
SR <- as.data.frame(sapply(SR, function(x) replace(x, x %in% val1, 0)))
#Assign risk level
SR<-SR %>% mutate(risk_level = case_when(as.numeric(DSTHOP30 != 98 & DSTHOP30<3) + as.numeric(mhsuithk==1)+
as.numeric(mhsuipln == 1) + as.numeric(mhsuitry == 1) +
as.numeric(K6SCMON > 16) + as.numeric(SMIPP_U > 0.65) >= 3 ~"high",
as.numeric(DSTHOP30 != 98 & DSTHOP30 == 3) + as.numeric(mhsuithk == 1) +
as.numeric(mhsuipln == 1 | mhsuitry == 1) +
as.numeric(K6SCMON <= 16 & K6SCMON >= 8) +
as.numeric(SMIPP_U <= 0.65 & SMIPP_U > 0.3) >= 3 ~"moderate",
as.numeric(DSTHOP30 != 98 & DSTHOP30>3) + as.numeric(K6SCMON < 8) +
as.numeric(mhsuithk== 1 | mhsuipln == 1 | mhsuitry == 1) +
as.numeric(SMIPP_U <= 0.3) >= 1 ~ "low"))
SR$risk_level[is.na(SR$risk_level)] <- "no"
#Combine treatment from multiple facilities into a single var
val2 = c(985,994,997,998,999)
SR[7:18] <- sapply(SR[7:18], function(x) replace(x, x %in% val2, 0))
SR$inpatient<-rowSums(SR[7:12])
SR$outpatient <- rowSums(SR[13:18])
#### Caluculate risk score
SR_reg<-SR
## Classification
SR<-SR[c(-1:-18)]
SR<-SR[c(1:11, 13, 14, 12)] #change order of the column
SR<-na.omit(SR[c(1,3:14)]) #omit the variable with too much NA and all rows w/ NA (drop ASDSOVL2)
SR[c(1,3,4,6,7,9,10,13)] <- lapply(SR[c(1,3,4,6,7,9,10,13)], factor)
#### Training Testing set
set.seed(1)
index = createDataPartition(y=SR$risk_level, p=0.7, list=FALSE)
sr_train =SR[index,]
sr_test = SR[-index,]
#Attempts on different algorithms---Model Training
ctrl = trainControl(method="repeatedcv", number = 10, repeats = 5)
#Decision Tress
ctrl_dt = trainControl(method="repeatedcv",number=10, repeats=5)
dtree = train(risk_level~ ., data = sr_train, method = "rpart", trControl = ctrl_dt,
tuneLength = 30)
SR<-SuicideRisk %>%
dplyr::select(DSTHOP30, mhsuithk, mhsuipln, mhsuitry, K6SCMON, SMIPP_U, AUNMPSY2, AUNMPGE2, AUNMMED2, AUNMAHS2, AUNMRES2,
AUNMSFA2, AUNMMEN2, AUNMTHE2, AUNMDOC2, AUNMCLN2, AUNMDTM2, AUNMOTO2, amdelt, ASDSOVL2, CATAG6, irmarit,
irsex, IREDUHIGHST2, NEWRACE2, wrkdpstyr, income, ALCWD2SX, alcndmor)
library(dplyr)
library(ggplot2)
library(caret)
library(MASS)
library(rpart)
library(rpart.plot)
library(tensorflow)
library(keras)
#Assign risk level
SR<-SR %>% mutate(risk_level = case_when(as.numeric(DSTHOP30 != 98 & DSTHOP30<3) + as.numeric(mhsuithk==1)+
as.numeric(mhsuipln == 1) + as.numeric(mhsuitry == 1) +
as.numeric(K6SCMON > 16) + as.numeric(SMIPP_U > 0.65) >= 3 ~"high",
as.numeric(DSTHOP30 != 98 & DSTHOP30 == 3) + as.numeric(mhsuithk == 1) +
as.numeric(mhsuipln == 1 | mhsuitry == 1) +
as.numeric(K6SCMON <= 16 & K6SCMON >= 8) +
as.numeric(SMIPP_U <= 0.65 & SMIPP_U > 0.3) >= 3 ~"moderate",
as.numeric(DSTHOP30 != 98 & DSTHOP30>3) + as.numeric(K6SCMON < 8) +
as.numeric(mhsuithk== 1 | mhsuipln == 1 | mhsuitry == 1) +
as.numeric(SMIPP_U <= 0.3) >= 1 ~ "low"))
SR$risk_level[is.na(SR$risk_level)] <- "no"
#Combine treatment from multiple facilities into a single var
val2 = c(985,994,997,998,999)
SR[7:18] <- sapply(SR[7:18], function(x) replace(x, x %in% val2, 0))
SR$inpatient<-rowSums(SR[7:12])
SR$outpatient <- rowSums(SR[13:18])
#### Caluculate risk score
SR_reg<-SR
## Classification
SR<-SR[c(-1:-18)]
SR<-SR[c(1:11, 13, 14, 12)] #change order of the column
SR<-na.omit(SR[c(1,3:14)]) #omit the variable with too much NA and all rows w/ NA (drop ASDSOVL2)
SR[c(1,3,4,6,7,9,10,13)] <- lapply(SR[c(1,3,4,6,7,9,10,13)], factor)
#### Training Testing set
set.seed(1)
index = createDataPartition(y=SR$risk_level, p=0.7, list=FALSE)
sr_train =SR[index,]
sr_test = SR[-index,]
#Attempts on different algorithms---Model Training
ctrl = trainControl(method="repeatedcv", number = 10, repeats = 5)
#Decision Tress
ctrl_dt = trainControl(method="repeatedcv",number=10, repeats=5)
dtree = train(risk_level~., data = sr_train, method = "rpart", trControl = ctrl_dt,
tuneLength = 30)
library(caret)
dtree = train(risk_level~., data = sr_train, method = "rpart", trControl = ctrl_dt,tuneLength = 30)
detach(package:Rcpp,unload=TRUE)
detach("package:Rcpp",unload=TRUE)
detach("package:rcpp",unload=TRUE)
